Support Vector Machines (SVM) is a supervised machimachine learning algorithm that can be used for both classification and regression tasks. In the context of classification, SVM aims to find the optimal hyperplane that separates the different classes in the feature space.

Here are the key concepts and steps involved in SVM for iris flower classification:

Data Preparation: Load the iris dataset, which contains samples of iris flowers with features such as sepal length, sepal width, petal length, and petal width. Split the dataset into training and testing sets.

Feature Scaling: It's important to scale the features so that they have similar ranges. This helps in preventing certain features from dominating the learning process. Common scaling techniques include standardization (mean = 0, standard deviation = 1) or normalization (scaling to a specific range, e.g., [0, 1]).

Kernel Function: SVM can handle linearly separable as well as non-linearly separable data by mapping the original feature space to a higher-dimensional space using a kernel function. The choice of kernel function depends on the characteristics of the dataset. Common kernel functions include linear, polynomial, and radial basis function (RBF).

Margin Maximization: SVM aims to find the hyperplane that maximizes the margin between the classes. The margin is the distance between the hyperplane and the nearest data points from each class (called support vectors). Maximizing the margin helps in improving the generalization of the model.

Optimization: The SVM algorithm formulates the problem as an optimization task. It aims to minimize the classification error while maximizing the margin. The optimization problem involves solving a quadratic programming (QP) problem to find the optimal weights and biases that define the hyperplane.

Soft Margin: In some cases, the data may not be perfectly separable with a hyperplane. SVM allows for a soft margin, which permits some misclassifications in the training data. This is controlled by a hyperparameter called C, which determines the trade-off between maximizing the margin and allowing misclassifications.

Classification: Once the SVM model is trained, it can be used to classify new, unseen samples. The model assigns a class label based on which side of the hyperplane the sample lies.

SVM has proven to be effective for iris flower classification due to its ability to handle both linearly and non-linearly separable data. The choice of kernel function and hyperparameter tuning plays a crucial role in achieving good classification performance.

SOURCE CODE IN PYTHON OF THE PROJECT:

import numpy as np
import seaborn as sns
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
df.pd.read_csv(r""C:\Users\gandi\Downloads\iris.data.csv")
df.head()
df.tail()
df.isnull().sum()
df.info()
plt.boxplot(df['5.1'])
plt.boxplot(df['3.5'])
plt.boxplot(df['1.4'])
plt.boxplot(df['0.2'])
sns.heatmap(df.corr())
sns.pairplot(df,hue='Iris-setosa')
df.drop('id',axis=2,inplace=True)
sp={"Iris-setosa":1,"Iris-Versicolor":2,"Iris-virginica":3}
df.Iris-setosa=[sp[i]for i in df.Iris-setosa]
df
X = df.iloc[:,2]
X
y = df.iloc[:,2]
y
X=df['5.1'].values.reshape(-1,1)
print(X)
y=df['5.1'].values.reshape(-1,1)
print(y)
X = df.drop('Iris-setosa', axis = 1)
y = df['Iris-setosa']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
from sklearn.svm import SVC
svm = SVC()
svm.fit(X_train,y_train)
setosa = df[df['Iris-setosa'] == 'Iris-setosa']
sns.kdeplot( setosa['5.1'], setosa['3.5'], cmap='PuRd', shade=True, shade_lowest=False)
versicolor = df[df['Iris-setosa'] == 'Iris-versicolor']
sns.kdeplot( versicolor['5.1'], versicolor['3.5'], cmap='inferno', shade=False, shade_lowest=True)
virginica = df[df['Iris-setosa'] == 'Iris-virginica']
sns.histplot( virginica['5.1'], kde=True)
pred = svm.predict(X_test)
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, pred))
from sklearn.metrics import classification_report
print(classification_report(y_test,pred))
accuracy=accuracy_score(y_test,pred)
print("accuracy:",accuracy)
svm.score(X_test,y_test)

And the SVM score after testing is '0.9555555555555556'
